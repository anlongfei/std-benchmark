\documentclass{sig-alternate}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{graphviz}
\usepackage{auto-pst-pdf}
\usepackage{etoolbox}
\usepackage{flushend}
\usepackage{needspace}

\makeatletter
\preto{\@verbatim}{\topsep=1pt \partopsep=0pt}
\makeatother

\pagenumbering{arabic}

\begin{document}
\def \GCC {GCC}
\def \LLVM {LLVM}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\title{Benchmarking C/C++ standard libraries}

\toappear{
   \hrule \vspace{5pt}
   Some conf
}
\numberofauthors{3}

\author{
\alignauthor
Aditya Kumar\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{aditya.k7@samsung.com}
\and
\alignauthor
Sebastian\\
       \affaddr{Samsung Austin R\&D Center}\\
       \email{ancd}
}

\maketitle
\begin{abstract}
We present a systematic analysis of C and C++ standard libraries. The goal
here is to enable each programmer make informed decision about the functionality
one is using and not just rely on common wisdom. We make it very easy to know
the internals of the standard libraries. There are benchmark analyses available
online but those are in bits and pieces. We present a comprehensize infrastructure
where a large subset of standard library is covered. This also enables someone
to run their own configuration of test by adding just few lines of code. This also
allows the programmer to compare which compiler toolchain generates better code
in terms of performance, code-size etc. such that they can choose the right
toolchain for their application. The comparative analysis of multiple toolchains
also enabled us to improve the underperforming library functions.
\end{abstract}

\section{Introduction}
Why a systematic analysis is important.

The main contributions of this paper are:
\begin{itemize}
\item a benchmark suite for C/C++ standard library
\item ability to compare compiler performance for standard libraries
\item identifying slower implementation in standard library
\item investigating whether C++11/14 really makes your code faster (for
standard libraries) at -O0, -O3
\end{itemize}

\section{Related Work}
Several bits and pieces of benchmarking available online.
Bjarne's channel9 talk \cite{stroustrup2012}.
He talks about std::list vs. std::vector, in fact there are several
analyses online, all of them establish std::vector as a better choice
over std::list. But is std::vector the best sequential data structure?
Our experiments indicate that std::deque may be better in many cases \ref{sec:experiments}.

clrs \cite{clrs}

\newpage

\subsection{Layout of the project}
Structure

\subsection{Illustrative Example} \label{subsec:example}
How to add a single benchmark \cite{googlebench}
\newpage


\section{Performance problems due to C++ standardese}
char_traits<char>::find has to check if both the pointer to the string
and the number of characters to analyze, because if both are zero then the result is valid (zero).


\begin{verbatim}
static const char_type*
find(const char_type* __s, size_t __n, const char_type& __a)
{
  if (__n == 0)
    return 0;
  return memchr(__s, __a, __n);
}
\end{verbatim}

A pointer to the first character in the range specified by [p, p + count) that compares equal to ch, or NULL if not found.


\section{Experimental Results and discussion}
\label{sec:experiments}

\subsection{Benchmark results comparison across toolchains}

\subsection{Time complexity results}

\subsection{C vs C++ algorithms}
string::find vs. strstr.

We present the results we got on x86-64 as well as aarch64 machines.

\section{compiler vs programmer}
We created test to figure out simple patterns which could be converted to
standard library functions by the compiler but did not in some cases. This is mostly
because of aliasing ambiguities and how the programmer can void them.
In the benchmark compiler.vs.programmer/memory.bench.cpp, we have program like:

\begin{verbatim}
const char* __attribute__ ((noinline))
assign(const char *beg, const char *end, char *dest) {
  while (beg != end)
    *dest++ = *beg++;
  return beg;
}
\end{verbatim}

This is a very common pattern found in many codebases including C++ standard libraries e.g.,
libcxx:locale.cpp:const char*ctype<char>::do\_widen,

\begin{verbatim}
const char*
ctype<char>::do_widen(const char* low, const char* high,
                      char_type* dest) const
{
    for (; low != high; ++low, ++dest)
        *dest = *low;
    return low;
}
\end{verbatim}

The dest never aliases with the low or the high pointer but the compiler fails to convert
this to memcpy, because it cannot figure out low, high, and dest are not aliases of each other.
It might be able to figure out if they were inlined in the caller but this function is in a .cpp
file and hence the caller will not see it. This function gets called over and over again
each time you invoke std::stringstream to parse token of integers from a character stream.

Just adding \_\_restrict\_\_ would solve this problem, for example in the function assign\_res from\\
std-benchmark/compiler.vs.programmer/memory.bench.cpp there is another function which shows the
usage in this case.

\begin{verbatim}
const char* __attribute__ ((noinline))
assign_res(const char * __restrict__ beg,
           const char * __restrict__ end,
           char *__restrict__ dest) {
  while (beg != end)
    *dest++ = *beg++;
  return beg;
}
\end{verbatim}

This function runs twice as fast as the one without restrict.

\begin{verbatim}
Benchmark                        Time           CPU Iterations
--------------------------------------------------------------
BM_prog_memcpy/32                5 ns          5 ns  143049157
BM_prog_memcpy/64                6 ns          6 ns  117543415
BM_prog_memcpy/128               8 ns          8 ns   87350103
BM_prog_memcpy/256              12 ns         12 ns   57677864
BM_prog_memcpy/512              20 ns         20 ns   34332565
BM_prog_memcpy/1024             36 ns         36 ns   19396612
BM_compiler_memcpy/32            4 ns          4 ns  181115627
BM_compiler_memcpy/64            4 ns          4 ns  169701384
BM_compiler_memcpy/128           6 ns          6 ns  111126103
BM_compiler_memcpy/256           6 ns          6 ns  122750774
BM_compiler_memcpy/512           8 ns          8 ns   91138876
BM_compiler_memcpy/1024         11 ns         11 ns   62055274
BM_memcpy/32                     3 ns          3 ns  246831272
BM_memcpy/64                     3 ns          3 ns  226251314
BM_memcpy/128                    6 ns          6 ns  124278117
BM_memcpy/256                    5 ns          5 ns  150856758
BM_memcpy/512                    6 ns          6 ns  114059692
BM_memcpy/1024                  10 ns         10 ns   69498277
\end{verbatim}


\section{Timing and Limitations}
Timing and Limitations
\subsection{Limitations of the time-complexity measurement}

\section{Conclusion and Future Work}

\bibliographystyle{abbrv}
{\small
\bibliography{Bibliography}
}
\end{document}
